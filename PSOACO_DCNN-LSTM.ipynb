{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PlAM-CJcYtH",
        "outputId": "b334a59d-036a-4d0a-cfef-ce61013eaef8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-1-47fba9c68164>:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  y_train = pd.read_csv(\"/content/drive/MyDrive/UCI HAR Dataset/UCI HAR Dataset/train/y_train.txt\", delim_whitespace=True, header=None).values.ravel()\n",
            "<ipython-input-1-47fba9c68164>:8: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  y_test = pd.read_csv(\"/content/drive/MyDrive/UCI HAR Dataset/UCI HAR Dataset/test/y_test.txt\", delim_whitespace=True, header=None).values.ravel()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ACO Iteration 1/47: Selected feature 48\n",
            "ACO Iteration 2/47: Selected feature 10\n",
            "ACO Iteration 3/47: Selected feature 48\n",
            "ACO Iteration 4/47: Selected feature 18\n",
            "ACO Iteration 5/47: Selected feature 63\n",
            "ACO Iteration 6/47: Selected feature 20\n",
            "ACO Iteration 7/47: Selected feature 61\n",
            "ACO Iteration 8/47: Selected feature 10\n",
            "ACO Iteration 9/47: Selected feature 76\n",
            "ACO Iteration 10/47: Selected feature 40\n",
            "ACO Iteration 11/47: Selected feature 5\n",
            "ACO Iteration 12/47: Selected feature 76\n",
            "ACO Iteration 13/47: Selected feature 56\n",
            "ACO Iteration 14/47: Selected feature 27\n",
            "ACO Iteration 15/47: Selected feature 6\n",
            "ACO Iteration 16/47: Selected feature 54\n",
            "ACO Iteration 17/47: Selected feature 79\n",
            "ACO Iteration 18/47: Selected feature 2\n",
            "ACO Iteration 19/47: Selected feature 59\n",
            "ACO Iteration 20/47: Selected feature 5\n",
            "ACO Iteration 21/47: Selected feature 36\n",
            "ACO Iteration 22/47: Selected feature 51\n",
            "ACO Iteration 23/47: Selected feature 56\n",
            "ACO Iteration 24/47: Selected feature 47\n",
            "ACO Iteration 25/47: Selected feature 15\n",
            "ACO Iteration 26/47: Selected feature 8\n",
            "ACO Iteration 27/47: Selected feature 17\n",
            "ACO Iteration 28/47: Selected feature 59\n",
            "ACO Iteration 29/47: Selected feature 60\n",
            "ACO Iteration 30/47: Selected feature 30\n",
            "ACO Iteration 31/47: Selected feature 62\n",
            "ACO Iteration 32/47: Selected feature 7\n",
            "ACO Iteration 33/47: Selected feature 14\n",
            "ACO Iteration 34/47: Selected feature 45\n",
            "ACO Iteration 35/47: Selected feature 19\n",
            "ACO Iteration 36/47: Selected feature 82\n",
            "ACO Iteration 37/47: Selected feature 50\n",
            "ACO Iteration 38/47: Selected feature 56\n",
            "ACO Iteration 39/47: Selected feature 0\n",
            "ACO Iteration 40/47: Selected feature 11\n",
            "ACO Iteration 41/47: Selected feature 67\n",
            "ACO Iteration 42/47: Selected feature 68\n",
            "ACO Iteration 43/47: Selected feature 13\n",
            "ACO Iteration 44/47: Selected feature 48\n",
            "ACO Iteration 45/47: Selected feature 30\n",
            "ACO Iteration 46/47: Selected feature 25\n",
            "ACO Iteration 47/47: Selected feature 27\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[48,\n",
              " 10,\n",
              " 48,\n",
              " 18,\n",
              " 63,\n",
              " 20,\n",
              " 61,\n",
              " 10,\n",
              " 76,\n",
              " 40,\n",
              " 5,\n",
              " 76,\n",
              " 56,\n",
              " 27,\n",
              " 6,\n",
              " 54,\n",
              " 79,\n",
              " 2,\n",
              " 59,\n",
              " 5,\n",
              " 36,\n",
              " 51,\n",
              " 56,\n",
              " 47,\n",
              " 15,\n",
              " 8,\n",
              " 17,\n",
              " 59,\n",
              " 60,\n",
              " 30,\n",
              " 62,\n",
              " 7,\n",
              " 14,\n",
              " 45,\n",
              " 19,\n",
              " 82,\n",
              " 50,\n",
              " 56,\n",
              " 0,\n",
              " 11,\n",
              " 67,\n",
              " 68,\n",
              " 13,\n",
              " 48,\n",
              " 30,\n",
              " 25,\n",
              " 27]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "y_train = pd.read_csv(\"/content/drive/MyDrive/UCI HAR Dataset/UCI HAR Dataset/train/y_train.txt\", delim_whitespace=True, header=None).values.ravel()\n",
        "y_test = pd.read_csv(\"/content/drive/MyDrive/UCI HAR Dataset/UCI HAR Dataset/test/y_test.txt\", delim_whitespace=True, header=None).values.ravel()\n",
        "\n",
        "\n",
        "# Combine training and testing data for PCA\n",
        "final_df1 = pd.read_csv(r\"/content/drive/MyDrive/geneticalgooutput.csv\")\n",
        "final_df1 = final_df1.iloc[:, :89]\n",
        "y_combined_df = pd.concat([pd.Series(y_train), pd.Series(y_test)])\n",
        "\n",
        "\n",
        "def calculate_similarity(selected_features, ants):\n",
        "    num_features = len(selected_features)\n",
        "    if num_features == 0:\n",
        "        return np.zeros(len(ants[0]))\n",
        "    similarity = np.sum(ants[:, selected_features], axis=0) / num_features\n",
        "    similarity = np.pad(similarity, (0, len(ants[0]) - num_features), 'constant')\n",
        "    return similarity\n",
        "\n",
        "def aco_feature_selection(num_features, num_ants, num_iterations_aco, X, y):\n",
        "    pheromones = np.ones(num_features)\n",
        "    selected_features = []\n",
        "    temporary=[i for i in range(10299)]\n",
        "    ants = X[temporary,:]\n",
        "    ant_fitness1 = np.array([np.mean(ant) for ant in ants])\n",
        "    for aco_iteration in range(num_iterations_aco):\n",
        "        ant_fitness=[]\n",
        "        random_indices=np.random.choice([i for i in range(10299)],size=num_ants)\n",
        "        for i in random_indices:\n",
        "            ant_fitness.append(ant_fitness1[i])\n",
        "\n",
        "        best_ant_index = np.argmax(ant_fitness)\n",
        "\n",
        "        if 0 <= best_ant_index < 10299:\n",
        "            best_ant_position = ants[best_ant_index].copy()\n",
        "\n",
        "            similarity = calculate_similarity(selected_features, ants)\n",
        "            for feature_i in range(num_features):\n",
        "                pheromones[feature_i] += ant_fitness[best_ant_index] * (1 - similarity[feature_i])\n",
        "\n",
        "            pheromone_probs = np.exp(pheromones) / np.sum(np.exp(pheromones))\n",
        "\n",
        "            selected_feature = np.random.choice(np.arange(num_features), p=pheromone_probs)\n",
        "            selected_features.append(selected_feature)  # Increment by 1 to match original feature indices\n",
        "\n",
        "            print(f\"ACO Iteration {aco_iteration + 1}/{num_iterations_aco}: Selected feature {selected_feature}\")\n",
        "        else:\n",
        "            print(\"Invalid best_ant_index:\", best_ant_index)\n",
        "\n",
        "    return selected_features\n",
        "\n",
        "\n",
        "\n",
        "X = final_df1\n",
        "y = y_combined_df\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_final = X_scaled[:]\n",
        "\n",
        "selected_features_aco = aco_feature_selection(X_final.shape[1], num_ants=200, num_iterations_aco=47, X=X_final, y=y)\n",
        "selected_features_aco"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EpaviU9deAdr"
      },
      "outputs": [],
      "source": [
        "X_final = X_scaled[:,selected_features_aco]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "HK8r0HTueHL2",
        "outputId": "3c40b309-afc9-4800-94eb-37e42bda52a0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X_final"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-c0006d3f-31cc-48f9-9b5e-ab932577501c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.344950</td>\n",
              "      <td>-0.868773</td>\n",
              "      <td>-0.344950</td>\n",
              "      <td>-0.771213</td>\n",
              "      <td>-0.900840</td>\n",
              "      <td>-0.849321</td>\n",
              "      <td>-0.374992</td>\n",
              "      <td>-0.868773</td>\n",
              "      <td>-2.042964</td>\n",
              "      <td>1.485495</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.706356</td>\n",
              "      <td>-1.731883</td>\n",
              "      <td>-0.936732</td>\n",
              "      <td>0.230048</td>\n",
              "      <td>-0.053390</td>\n",
              "      <td>0.918871</td>\n",
              "      <td>-0.344950</td>\n",
              "      <td>1.126231</td>\n",
              "      <td>-1.206949</td>\n",
              "      <td>-3.891042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.406656</td>\n",
              "      <td>-0.884263</td>\n",
              "      <td>-0.406656</td>\n",
              "      <td>-0.770620</td>\n",
              "      <td>-0.900840</td>\n",
              "      <td>-0.863017</td>\n",
              "      <td>-0.389608</td>\n",
              "      <td>-0.884263</td>\n",
              "      <td>-1.405613</td>\n",
              "      <td>1.269000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.726777</td>\n",
              "      <td>-1.731546</td>\n",
              "      <td>-0.902697</td>\n",
              "      <td>-0.615071</td>\n",
              "      <td>0.784602</td>\n",
              "      <td>0.908664</td>\n",
              "      <td>-0.406656</td>\n",
              "      <td>0.571360</td>\n",
              "      <td>-1.122341</td>\n",
              "      <td>-1.772857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.442169</td>\n",
              "      <td>-0.876139</td>\n",
              "      <td>-0.442169</td>\n",
              "      <td>-0.769600</td>\n",
              "      <td>-0.900840</td>\n",
              "      <td>-0.857393</td>\n",
              "      <td>-0.415337</td>\n",
              "      <td>-0.876139</td>\n",
              "      <td>-0.967963</td>\n",
              "      <td>1.347501</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.773143</td>\n",
              "      <td>-1.731210</td>\n",
              "      <td>-0.902697</td>\n",
              "      <td>-2.456876</td>\n",
              "      <td>2.520361</td>\n",
              "      <td>0.892681</td>\n",
              "      <td>-0.442169</td>\n",
              "      <td>0.184032</td>\n",
              "      <td>-1.122341</td>\n",
              "      <td>-0.788590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.303938</td>\n",
              "      <td>-0.876139</td>\n",
              "      <td>-0.303938</td>\n",
              "      <td>-0.770205</td>\n",
              "      <td>-0.900840</td>\n",
              "      <td>-0.857533</td>\n",
              "      <td>-0.273539</td>\n",
              "      <td>-0.876139</td>\n",
              "      <td>0.358887</td>\n",
              "      <td>1.077732</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.769766</td>\n",
              "      <td>-1.730874</td>\n",
              "      <td>-0.968103</td>\n",
              "      <td>-2.228321</td>\n",
              "      <td>2.380405</td>\n",
              "      <td>0.892681</td>\n",
              "      <td>-0.303938</td>\n",
              "      <td>0.401738</td>\n",
              "      <td>-2.048469</td>\n",
              "      <td>-0.136505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.395261</td>\n",
              "      <td>-0.883152</td>\n",
              "      <td>-0.395261</td>\n",
              "      <td>-0.771156</td>\n",
              "      <td>-0.900840</td>\n",
              "      <td>-0.859827</td>\n",
              "      <td>-0.396832</td>\n",
              "      <td>-0.883152</td>\n",
              "      <td>0.935553</td>\n",
              "      <td>1.321739</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.770272</td>\n",
              "      <td>-1.730537</td>\n",
              "      <td>-0.943148</td>\n",
              "      <td>-1.305973</td>\n",
              "      <td>1.443147</td>\n",
              "      <td>0.908067</td>\n",
              "      <td>-0.395261</td>\n",
              "      <td>1.129075</td>\n",
              "      <td>-1.639764</td>\n",
              "      <td>-0.970273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10294</th>\n",
              "      <td>0.376249</td>\n",
              "      <td>1.210539</td>\n",
              "      <td>0.376249</td>\n",
              "      <td>0.384289</td>\n",
              "      <td>0.510479</td>\n",
              "      <td>0.337984</td>\n",
              "      <td>0.440705</td>\n",
              "      <td>1.210539</td>\n",
              "      <td>0.735092</td>\n",
              "      <td>-0.332102</td>\n",
              "      <td>...</td>\n",
              "      <td>0.748778</td>\n",
              "      <td>1.730537</td>\n",
              "      <td>0.517569</td>\n",
              "      <td>-0.127378</td>\n",
              "      <td>0.098902</td>\n",
              "      <td>-0.477836</td>\n",
              "      <td>0.376249</td>\n",
              "      <td>-0.769440</td>\n",
              "      <td>0.984414</td>\n",
              "      <td>0.043661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10295</th>\n",
              "      <td>0.277969</td>\n",
              "      <td>1.210539</td>\n",
              "      <td>0.277969</td>\n",
              "      <td>0.856951</td>\n",
              "      <td>0.490641</td>\n",
              "      <td>0.389202</td>\n",
              "      <td>0.311346</td>\n",
              "      <td>1.210539</td>\n",
              "      <td>0.035068</td>\n",
              "      <td>-0.188236</td>\n",
              "      <td>...</td>\n",
              "      <td>0.714089</td>\n",
              "      <td>1.730874</td>\n",
              "      <td>0.961959</td>\n",
              "      <td>-0.007074</td>\n",
              "      <td>-0.013270</td>\n",
              "      <td>-0.310328</td>\n",
              "      <td>0.277969</td>\n",
              "      <td>-0.557853</td>\n",
              "      <td>0.735797</td>\n",
              "      <td>-0.522792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10296</th>\n",
              "      <td>0.213774</td>\n",
              "      <td>0.880306</td>\n",
              "      <td>0.213774</td>\n",
              "      <td>0.669623</td>\n",
              "      <td>0.343827</td>\n",
              "      <td>0.426386</td>\n",
              "      <td>0.183323</td>\n",
              "      <td>0.880306</td>\n",
              "      <td>-0.373917</td>\n",
              "      <td>0.044241</td>\n",
              "      <td>...</td>\n",
              "      <td>0.708206</td>\n",
              "      <td>1.731210</td>\n",
              "      <td>0.961959</td>\n",
              "      <td>0.080272</td>\n",
              "      <td>-0.087762</td>\n",
              "      <td>-0.344355</td>\n",
              "      <td>0.213774</td>\n",
              "      <td>-0.345541</td>\n",
              "      <td>0.884723</td>\n",
              "      <td>-1.030175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10297</th>\n",
              "      <td>-0.095337</td>\n",
              "      <td>0.880306</td>\n",
              "      <td>-0.095337</td>\n",
              "      <td>0.162166</td>\n",
              "      <td>0.198601</td>\n",
              "      <td>0.355007</td>\n",
              "      <td>-0.085267</td>\n",
              "      <td>0.880306</td>\n",
              "      <td>0.050097</td>\n",
              "      <td>0.054367</td>\n",
              "      <td>...</td>\n",
              "      <td>0.681904</td>\n",
              "      <td>1.731546</td>\n",
              "      <td>0.711316</td>\n",
              "      <td>-0.539951</td>\n",
              "      <td>0.548295</td>\n",
              "      <td>-0.470604</td>\n",
              "      <td>-0.095337</td>\n",
              "      <td>-0.407453</td>\n",
              "      <td>1.386772</td>\n",
              "      <td>-1.325299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10298</th>\n",
              "      <td>-0.116289</td>\n",
              "      <td>0.659399</td>\n",
              "      <td>-0.116289</td>\n",
              "      <td>0.226944</td>\n",
              "      <td>0.347061</td>\n",
              "      <td>0.202888</td>\n",
              "      <td>-0.092048</td>\n",
              "      <td>0.659399</td>\n",
              "      <td>0.382083</td>\n",
              "      <td>-0.122185</td>\n",
              "      <td>...</td>\n",
              "      <td>0.690000</td>\n",
              "      <td>1.731883</td>\n",
              "      <td>0.809835</td>\n",
              "      <td>-0.327348</td>\n",
              "      <td>0.316936</td>\n",
              "      <td>-0.794923</td>\n",
              "      <td>-0.116289</td>\n",
              "      <td>-0.837652</td>\n",
              "      <td>1.137309</td>\n",
              "      <td>-0.653513</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10299 rows × 47 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c0006d3f-31cc-48f9-9b5e-ab932577501c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c0006d3f-31cc-48f9-9b5e-ab932577501c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c0006d3f-31cc-48f9-9b5e-ab932577501c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1a1ba211-822f-4507-8d56-a238f71e63c5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1a1ba211-822f-4507-8d56-a238f71e63c5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1a1ba211-822f-4507-8d56-a238f71e63c5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "             0         1         2         3         4         5         6   \\\n",
              "0     -0.344950 -0.868773 -0.344950 -0.771213 -0.900840 -0.849321 -0.374992   \n",
              "1     -0.406656 -0.884263 -0.406656 -0.770620 -0.900840 -0.863017 -0.389608   \n",
              "2     -0.442169 -0.876139 -0.442169 -0.769600 -0.900840 -0.857393 -0.415337   \n",
              "3     -0.303938 -0.876139 -0.303938 -0.770205 -0.900840 -0.857533 -0.273539   \n",
              "4     -0.395261 -0.883152 -0.395261 -0.771156 -0.900840 -0.859827 -0.396832   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "10294  0.376249  1.210539  0.376249  0.384289  0.510479  0.337984  0.440705   \n",
              "10295  0.277969  1.210539  0.277969  0.856951  0.490641  0.389202  0.311346   \n",
              "10296  0.213774  0.880306  0.213774  0.669623  0.343827  0.426386  0.183323   \n",
              "10297 -0.095337  0.880306 -0.095337  0.162166  0.198601  0.355007 -0.085267   \n",
              "10298 -0.116289  0.659399 -0.116289  0.226944  0.347061  0.202888 -0.092048   \n",
              "\n",
              "             7         8         9   ...        37        38        39  \\\n",
              "0     -0.868773 -2.042964  1.485495  ... -0.706356 -1.731883 -0.936732   \n",
              "1     -0.884263 -1.405613  1.269000  ... -0.726777 -1.731546 -0.902697   \n",
              "2     -0.876139 -0.967963  1.347501  ... -0.773143 -1.731210 -0.902697   \n",
              "3     -0.876139  0.358887  1.077732  ... -0.769766 -1.730874 -0.968103   \n",
              "4     -0.883152  0.935553  1.321739  ... -0.770272 -1.730537 -0.943148   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "10294  1.210539  0.735092 -0.332102  ...  0.748778  1.730537  0.517569   \n",
              "10295  1.210539  0.035068 -0.188236  ...  0.714089  1.730874  0.961959   \n",
              "10296  0.880306 -0.373917  0.044241  ...  0.708206  1.731210  0.961959   \n",
              "10297  0.880306  0.050097  0.054367  ...  0.681904  1.731546  0.711316   \n",
              "10298  0.659399  0.382083 -0.122185  ...  0.690000  1.731883  0.809835   \n",
              "\n",
              "             40        41        42        43        44        45        46  \n",
              "0      0.230048 -0.053390  0.918871 -0.344950  1.126231 -1.206949 -3.891042  \n",
              "1     -0.615071  0.784602  0.908664 -0.406656  0.571360 -1.122341 -1.772857  \n",
              "2     -2.456876  2.520361  0.892681 -0.442169  0.184032 -1.122341 -0.788590  \n",
              "3     -2.228321  2.380405  0.892681 -0.303938  0.401738 -2.048469 -0.136505  \n",
              "4     -1.305973  1.443147  0.908067 -0.395261  1.129075 -1.639764 -0.970273  \n",
              "...         ...       ...       ...       ...       ...       ...       ...  \n",
              "10294 -0.127378  0.098902 -0.477836  0.376249 -0.769440  0.984414  0.043661  \n",
              "10295 -0.007074 -0.013270 -0.310328  0.277969 -0.557853  0.735797 -0.522792  \n",
              "10296  0.080272 -0.087762 -0.344355  0.213774 -0.345541  0.884723 -1.030175  \n",
              "10297 -0.539951  0.548295 -0.470604 -0.095337 -0.407453  1.386772 -1.325299  \n",
              "10298 -0.327348  0.316936 -0.794923 -0.116289 -0.837652  1.137309 -0.653513  \n",
              "\n",
              "[10299 rows x 47 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_final=pd.DataFrame(X_final)\n",
        "X_final.head(10299)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "id": "6vauoDqseJ-0",
        "outputId": "417611ac-f086-47f4-fa4f-4a6cfc1beab9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting niapy\n",
            "  Downloading niapy-2.1.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting matplotlib<4.0.0,>=3.8.0 (from niapy)\n",
            "  Downloading matplotlib-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy<2.0.0,>=1.26.1 (from niapy)\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: openpyxl<4.0.0,>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from niapy) (3.1.2)\n",
            "Collecting pandas<3.0.0,>=2.1.1 (from niapy)\n",
            "  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.0->niapy) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.0->niapy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.0->niapy) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.0->niapy) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.0->niapy) (24.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.0->niapy) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.0->niapy) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.8.0->niapy) (2.8.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl<4.0.0,>=3.1.2->niapy) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=2.1.1->niapy) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=2.1.1->niapy) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.8.0->niapy) (1.16.0)\n",
            "Installing collected packages: numpy, pandas, matplotlib, niapy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.0.3\n",
            "    Uninstalling pandas-2.0.3:\n",
            "      Successfully uninstalled pandas-2.0.3\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.7.1\n",
            "    Uninstalling matplotlib-3.7.1:\n",
            "      Successfully uninstalled matplotlib-3.7.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.0.3, but you have pandas 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed matplotlib-3.8.4 niapy-2.1.0 numpy-1.26.4 pandas-2.2.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "1a0a7ef50899424e855684a33677477d",
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pip install niapy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9W04_I41eNKg"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from niapy.problems import Problem\n",
        "from niapy.task import Task\n",
        "from niapy.algorithms.basic import ParticleSwarmOptimization\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xb5Ot-gleTXK"
      },
      "outputs": [],
      "source": [
        "class SVMFeatureSelection(Problem):\n",
        "    def __init__(self, X_train, y_train, alpha=0.99):\n",
        "        super().__init__(dimension=X_train.shape[1], lower=[0] * X_train.shape[1], upper=[1] * X_train.shape[1])\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def _evaluate(self, x):\n",
        "        selected = x > 0.5\n",
        "        num_selected = selected.sum()\n",
        "        if num_selected == 0:\n",
        "            return 1.0\n",
        "        accuracy = cross_val_score(SVC(), self.X_train[:, selected], self.y_train, cv=2, n_jobs=-1).mean()\n",
        "        score = 1 - accuracy\n",
        "        num_features = self.X_train.shape[1]\n",
        "        return self.alpha * score + (1 - self.alpha) * (num_selected / num_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HUCPtoDpeVl3"
      },
      "outputs": [],
      "source": [
        "X_scaled = scaler.fit_transform(X_final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fOwArAeWeg2w"
      },
      "outputs": [],
      "source": [
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "y_encoded = np.ravel(y_encoded)\n",
        "X_tensor = torch.FloatTensor(X_scaled.astype('float32'))\n",
        "y_tensor = torch.LongTensor(y_encoded).ravel()\n",
        "\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "X_tensor, y_tensor = X_tensor.to(device), y_tensor.to(device)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBatkDNheiz2",
        "outputId": "aec29ba3-8c77-4974-f9f1-5f77017db753"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of selected features: 20\n",
            "Selected features: 1, 3, 8, 9, 10, 13, 15, 18, 19, 21, 22, 26, 32, 33, 34, 37, 38, 40, 42, 45\n",
            "Subset accuracy: 0.9247572815533981\n",
            "All Features Accuracy: 0.920873786407767\n"
          ]
        }
      ],
      "source": [
        "problem = SVMFeatureSelection(X_train, y_train)\n",
        "task = Task(problem, max_iters=100)\n",
        "algorithm = ParticleSwarmOptimization(population_size=10, seed=1234)\n",
        "best_features, best_fitness = algorithm.run(task)\n",
        "\n",
        "\n",
        "selected_features = best_features > 0.5\n",
        "print('Number of selected features:', selected_features.sum())\n",
        "print('Selected features:', ', '.join(map(str, np.where(selected_features)[0])))\n",
        "\n",
        "model_selected = SVC()\n",
        "model_all = SVC()\n",
        "\n",
        "model_selected.fit(X_train[:, selected_features], y_train)\n",
        "print('Subset accuracy:', model_selected.score(X_test[:, selected_features], y_test))\n",
        "\n",
        "model_all.fit(X_train, y_train)\n",
        "print('All Features Accuracy:', model_all.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "B2LwZXGzeknq",
        "outputId": "b1fcc834-15e7-4bf7-fd07-3a7a2a4fafee"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"X_temp\",\n  \"rows\": 10299,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0000485519384648,\n        \"min\": -0.9899513661151172,\n        \"max\": 2.722823651136467,\n        \"num_unique_values\": 7301,\n        \"samples\": [\n          -0.7264420601287089,\n          1.6447737763086323,\n          -0.8274347199191009\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 1,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0000485519384648,\n        \"min\": -0.7723002011806194,\n        \"max\": 15.103017226099665,\n        \"num_unique_values\": 10105,\n        \"samples\": [\n          -0.7715977977610493,\n          -0.7602307529767683,\n          0.599237767122694\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 2,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0000485519384648,\n        \"min\": -2.0600631487007925,\n        \"max\": 5.833649952489531,\n        \"num_unique_values\": 10298,\n        \"samples\": [\n          0.4080775459606579,\n          -0.21181215223635702,\n          -0.28928669661630646\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 3,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0000485519384648,\n        \"min\": -2.9311422817827317,\n        \"max\": 2.3875753785799354,\n        \"num_unique_values\": 10299,\n        \"samples\": [\n          -0.12008305829522274,\n          -1.9642226805185627,\n          0.9466246935257076\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 4,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.000048551938465,\n        \"min\": -0.9791952481719818,\n        \"max\": 3.0190816508006475,\n        \"num_unique_values\": 10297,\n        \"samples\": [\n          -0.9262987644884156,\n          -0.927212330777062,\n          -0.843658039410014\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 5,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0000485519384648,\n        \"min\": -4.485831287826972,\n        \"max\": 3.607146675688982,\n        \"num_unique_values\": 10299,\n        \"samples\": [\n          -0.8250708495742943,\n          -1.2741867608442337,\n          -0.9129855409091391\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 6,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0000485519384648,\n        \"min\": -2.7009708483555874,\n        \"max\": 2.6127217670139493,\n        \"num_unique_values\": 7967,\n        \"samples\": [\n          -0.7662795178324758,\n          -0.6045014627427797,\n          -0.479462836333473\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 7,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0000485519384648,\n        \"min\": -0.5574450551505477,\n        \"max\": 4.158856360900227,\n        \"num_unique_values\": 10290,\n        \"samples\": [\n          3.733293754544244,\n          1.5214094901012756,\n          -0.41117090041531346\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 8,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.000048551938465,\n        \"min\": -0.9791952481719818,\n        \"max\": 3.0190816508006475,\n        \"num_unique_values\": 10297,\n        \"samples\": [\n          -0.9262987644884156,\n          -0.927212330777062,\n          -0.843658039410014\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 9,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0000485519384648,\n        \"min\": -2.699129896036722,\n        \"max\": 2.755430142290831,\n        \"num_unique_values\": 8090,\n        \"samples\": [\n          2.104357710618341,\n          0.3601187107068981,\n          1.244549329942326\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 10,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0000485519384648,\n        \"min\": -2.299959577518497,\n        \"max\": 2.803050444831638,\n        \"num_unique_values\": 10299,\n        \"samples\": [\n          -0.7241415605900379,\n          0.4776774258890074,\n          0.3951762387215541\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 11,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0000485519384648,\n        \"min\": -0.7065270963190272,\n        \"max\": 7.389333590125513,\n        \"num_unique_values\": 9744,\n        \"samples\": [\n          0.3817902264785295,\n          -0.6887446192798586,\n          0.149336450222674\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 12,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0000485519384648,\n        \"min\": -4.101013340955146,\n        \"max\": 1.8016931849281026,\n        \"num_unique_values\": 7359,\n        \"samples\": [\n          0.8665289542496488,\n          0.8847118285131755,\n          0.7390572325511329\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 13,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0000485519384648,\n        \"min\": -0.5345995261341404,\n        \"max\": 22.916876112672096,\n        \"num_unique_values\": 10293,\n        \"samples\": [\n          -0.04155933728260247,\n          -0.16474102725250855,\n          -0.5202970574289238\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 14,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0000485519384648,\n        \"min\": -0.7055682502237608,\n        \"max\": 9.003758716325006,\n        \"num_unique_values\": 10233,\n        \"samples\": [\n          -0.7017481640754071,\n          0.08229319315808029,\n          -0.7052180833467124\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 15,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0000485519384648,\n        \"min\": -2.299959577518497,\n        \"max\": 2.803050444831638,\n        \"num_unique_values\": 10299,\n        \"samples\": [\n          -0.7241415605900379,\n          0.4776774258890074,\n          0.3951762387215541\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 16,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0000485519384648,\n        \"min\": -1.7318826391323854,\n        \"max\": 1.7318826391323854,\n        \"num_unique_values\": 10299,\n        \"samples\": [\n          0.22031134756879245,\n          -0.49141202869924544,\n          0.7436769304955727\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 17,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0000485519384648,\n        \"min\": -7.354749501738999,\n        \"max\": 2.179003323041735,\n        \"num_unique_values\": 10298,\n        \"samples\": [\n          -0.8110394984921773,\n          0.14003752986119025,\n          0.6911861164312063\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 18,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0000485519384648,\n        \"min\": -4.277693441078302,\n        \"max\": 1.3312775670293007,\n        \"num_unique_values\": 7323,\n        \"samples\": [\n          0.8875480284327677,\n          -1.3826393156505152,\n          0.8705663078086213\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 19,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0000485519384648,\n        \"min\": -2.2878935309209956,\n        \"max\": 3.145651371409207,\n        \"num_unique_values\": 8759,\n        \"samples\": [\n          -0.2092858210023686,\n          1.2413289339010183,\n          0.4443967088189755\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "X_temp"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-4188929f-9fac-4ddb-a482-4cf63f8ffe05\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.868773</td>\n",
              "      <td>-0.771213</td>\n",
              "      <td>-2.042964</td>\n",
              "      <td>1.485495</td>\n",
              "      <td>-0.945431</td>\n",
              "      <td>-3.891042</td>\n",
              "      <td>-0.371484</td>\n",
              "      <td>-0.499695</td>\n",
              "      <td>-0.945431</td>\n",
              "      <td>-0.411666</td>\n",
              "      <td>-0.706356</td>\n",
              "      <td>-0.706303</td>\n",
              "      <td>0.874510</td>\n",
              "      <td>-0.320117</td>\n",
              "      <td>-0.679412</td>\n",
              "      <td>-0.706356</td>\n",
              "      <td>-1.731883</td>\n",
              "      <td>0.230048</td>\n",
              "      <td>0.918871</td>\n",
              "      <td>-1.206949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.884263</td>\n",
              "      <td>-0.770620</td>\n",
              "      <td>-1.405613</td>\n",
              "      <td>1.269000</td>\n",
              "      <td>-0.929817</td>\n",
              "      <td>-1.772857</td>\n",
              "      <td>-0.349378</td>\n",
              "      <td>-0.506746</td>\n",
              "      <td>-0.929817</td>\n",
              "      <td>-0.411876</td>\n",
              "      <td>-0.726777</td>\n",
              "      <td>-0.706492</td>\n",
              "      <td>0.874510</td>\n",
              "      <td>-0.410863</td>\n",
              "      <td>-0.697827</td>\n",
              "      <td>-0.726777</td>\n",
              "      <td>-1.731546</td>\n",
              "      <td>-0.615071</td>\n",
              "      <td>0.908664</td>\n",
              "      <td>-1.122341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.876139</td>\n",
              "      <td>-0.769600</td>\n",
              "      <td>-0.967963</td>\n",
              "      <td>1.347501</td>\n",
              "      <td>-0.913598</td>\n",
              "      <td>-0.788590</td>\n",
              "      <td>-0.349378</td>\n",
              "      <td>-0.514226</td>\n",
              "      <td>-0.913598</td>\n",
              "      <td>-0.418336</td>\n",
              "      <td>-0.773143</td>\n",
              "      <td>-0.706402</td>\n",
              "      <td>0.864346</td>\n",
              "      <td>-0.450946</td>\n",
              "      <td>-0.702993</td>\n",
              "      <td>-0.773143</td>\n",
              "      <td>-1.731210</td>\n",
              "      <td>-2.456876</td>\n",
              "      <td>0.892681</td>\n",
              "      <td>-1.122341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.876139</td>\n",
              "      <td>-0.770205</td>\n",
              "      <td>0.358887</td>\n",
              "      <td>1.077732</td>\n",
              "      <td>-0.946015</td>\n",
              "      <td>-0.136505</td>\n",
              "      <td>-0.366495</td>\n",
              "      <td>-0.515988</td>\n",
              "      <td>-0.946015</td>\n",
              "      <td>-0.418336</td>\n",
              "      <td>-0.769766</td>\n",
              "      <td>-0.706428</td>\n",
              "      <td>0.864346</td>\n",
              "      <td>-0.316416</td>\n",
              "      <td>-0.703159</td>\n",
              "      <td>-0.769766</td>\n",
              "      <td>-1.730874</td>\n",
              "      <td>-2.228321</td>\n",
              "      <td>0.892681</td>\n",
              "      <td>-2.048469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.883152</td>\n",
              "      <td>-0.771156</td>\n",
              "      <td>0.935553</td>\n",
              "      <td>1.321739</td>\n",
              "      <td>-0.940846</td>\n",
              "      <td>-0.970273</td>\n",
              "      <td>-0.367818</td>\n",
              "      <td>-0.520570</td>\n",
              "      <td>-0.940846</td>\n",
              "      <td>-0.426723</td>\n",
              "      <td>-0.770272</td>\n",
              "      <td>-0.706489</td>\n",
              "      <td>0.866851</td>\n",
              "      <td>-0.395043</td>\n",
              "      <td>-0.704387</td>\n",
              "      <td>-0.770272</td>\n",
              "      <td>-1.730537</td>\n",
              "      <td>-1.305973</td>\n",
              "      <td>0.908067</td>\n",
              "      <td>-1.639764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10294</th>\n",
              "      <td>1.210539</td>\n",
              "      <td>0.384289</td>\n",
              "      <td>0.735092</td>\n",
              "      <td>-0.332102</td>\n",
              "      <td>0.738886</td>\n",
              "      <td>0.043661</td>\n",
              "      <td>-0.767326</td>\n",
              "      <td>-0.294744</td>\n",
              "      <td>0.738886</td>\n",
              "      <td>-0.731239</td>\n",
              "      <td>0.748778</td>\n",
              "      <td>0.326047</td>\n",
              "      <td>-0.593473</td>\n",
              "      <td>0.398894</td>\n",
              "      <td>0.647372</td>\n",
              "      <td>0.748778</td>\n",
              "      <td>1.730537</td>\n",
              "      <td>-0.127378</td>\n",
              "      <td>-0.477836</td>\n",
              "      <td>0.984414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10295</th>\n",
              "      <td>1.210539</td>\n",
              "      <td>0.856951</td>\n",
              "      <td>0.035068</td>\n",
              "      <td>-0.188236</td>\n",
              "      <td>1.076214</td>\n",
              "      <td>-0.522792</td>\n",
              "      <td>-0.756616</td>\n",
              "      <td>-0.302345</td>\n",
              "      <td>1.076214</td>\n",
              "      <td>-0.731239</td>\n",
              "      <td>0.714089</td>\n",
              "      <td>0.281338</td>\n",
              "      <td>-1.213350</td>\n",
              "      <td>0.291804</td>\n",
              "      <td>0.710708</td>\n",
              "      <td>0.714089</td>\n",
              "      <td>1.730874</td>\n",
              "      <td>-0.007074</td>\n",
              "      <td>-0.310328</td>\n",
              "      <td>0.735797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10296</th>\n",
              "      <td>0.880306</td>\n",
              "      <td>0.669623</td>\n",
              "      <td>-0.373917</td>\n",
              "      <td>0.044241</td>\n",
              "      <td>0.935694</td>\n",
              "      <td>-1.030175</td>\n",
              "      <td>-0.756616</td>\n",
              "      <td>-0.306312</td>\n",
              "      <td>0.935694</td>\n",
              "      <td>-0.718627</td>\n",
              "      <td>0.708206</td>\n",
              "      <td>0.212658</td>\n",
              "      <td>-1.213350</td>\n",
              "      <td>0.247496</td>\n",
              "      <td>0.531317</td>\n",
              "      <td>0.708206</td>\n",
              "      <td>1.731210</td>\n",
              "      <td>0.080272</td>\n",
              "      <td>-0.344355</td>\n",
              "      <td>0.884723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10297</th>\n",
              "      <td>0.880306</td>\n",
              "      <td>0.162166</td>\n",
              "      <td>0.050097</td>\n",
              "      <td>0.054367</td>\n",
              "      <td>0.560590</td>\n",
              "      <td>-1.325299</td>\n",
              "      <td>-0.694372</td>\n",
              "      <td>-0.286905</td>\n",
              "      <td>0.560590</td>\n",
              "      <td>-0.715810</td>\n",
              "      <td>0.681904</td>\n",
              "      <td>0.227469</td>\n",
              "      <td>-0.336173</td>\n",
              "      <td>-0.091354</td>\n",
              "      <td>0.673244</td>\n",
              "      <td>0.681904</td>\n",
              "      <td>1.731546</td>\n",
              "      <td>-0.539951</td>\n",
              "      <td>-0.470604</td>\n",
              "      <td>1.386772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10298</th>\n",
              "      <td>0.659399</td>\n",
              "      <td>0.226944</td>\n",
              "      <td>0.382083</td>\n",
              "      <td>-0.122185</td>\n",
              "      <td>0.629605</td>\n",
              "      <td>-0.653513</td>\n",
              "      <td>-0.689311</td>\n",
              "      <td>-0.284771</td>\n",
              "      <td>0.629605</td>\n",
              "      <td>-0.715810</td>\n",
              "      <td>0.690000</td>\n",
              "      <td>0.219802</td>\n",
              "      <td>-0.653277</td>\n",
              "      <td>-0.119239</td>\n",
              "      <td>0.829370</td>\n",
              "      <td>0.690000</td>\n",
              "      <td>1.731883</td>\n",
              "      <td>-0.327348</td>\n",
              "      <td>-0.794923</td>\n",
              "      <td>1.137309</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10299 rows × 20 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4188929f-9fac-4ddb-a482-4cf63f8ffe05')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4188929f-9fac-4ddb-a482-4cf63f8ffe05 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4188929f-9fac-4ddb-a482-4cf63f8ffe05');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9ed0740d-df22-474e-8a3c-ebef3f339f53\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9ed0740d-df22-474e-8a3c-ebef3f339f53')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9ed0740d-df22-474e-8a3c-ebef3f339f53 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "             0         1         2         3         4         5         6   \\\n",
              "0     -0.868773 -0.771213 -2.042964  1.485495 -0.945431 -3.891042 -0.371484   \n",
              "1     -0.884263 -0.770620 -1.405613  1.269000 -0.929817 -1.772857 -0.349378   \n",
              "2     -0.876139 -0.769600 -0.967963  1.347501 -0.913598 -0.788590 -0.349378   \n",
              "3     -0.876139 -0.770205  0.358887  1.077732 -0.946015 -0.136505 -0.366495   \n",
              "4     -0.883152 -0.771156  0.935553  1.321739 -0.940846 -0.970273 -0.367818   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "10294  1.210539  0.384289  0.735092 -0.332102  0.738886  0.043661 -0.767326   \n",
              "10295  1.210539  0.856951  0.035068 -0.188236  1.076214 -0.522792 -0.756616   \n",
              "10296  0.880306  0.669623 -0.373917  0.044241  0.935694 -1.030175 -0.756616   \n",
              "10297  0.880306  0.162166  0.050097  0.054367  0.560590 -1.325299 -0.694372   \n",
              "10298  0.659399  0.226944  0.382083 -0.122185  0.629605 -0.653513 -0.689311   \n",
              "\n",
              "             7         8         9         10        11        12        13  \\\n",
              "0     -0.499695 -0.945431 -0.411666 -0.706356 -0.706303  0.874510 -0.320117   \n",
              "1     -0.506746 -0.929817 -0.411876 -0.726777 -0.706492  0.874510 -0.410863   \n",
              "2     -0.514226 -0.913598 -0.418336 -0.773143 -0.706402  0.864346 -0.450946   \n",
              "3     -0.515988 -0.946015 -0.418336 -0.769766 -0.706428  0.864346 -0.316416   \n",
              "4     -0.520570 -0.940846 -0.426723 -0.770272 -0.706489  0.866851 -0.395043   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "10294 -0.294744  0.738886 -0.731239  0.748778  0.326047 -0.593473  0.398894   \n",
              "10295 -0.302345  1.076214 -0.731239  0.714089  0.281338 -1.213350  0.291804   \n",
              "10296 -0.306312  0.935694 -0.718627  0.708206  0.212658 -1.213350  0.247496   \n",
              "10297 -0.286905  0.560590 -0.715810  0.681904  0.227469 -0.336173 -0.091354   \n",
              "10298 -0.284771  0.629605 -0.715810  0.690000  0.219802 -0.653277 -0.119239   \n",
              "\n",
              "             14        15        16        17        18        19  \n",
              "0     -0.679412 -0.706356 -1.731883  0.230048  0.918871 -1.206949  \n",
              "1     -0.697827 -0.726777 -1.731546 -0.615071  0.908664 -1.122341  \n",
              "2     -0.702993 -0.773143 -1.731210 -2.456876  0.892681 -1.122341  \n",
              "3     -0.703159 -0.769766 -1.730874 -2.228321  0.892681 -2.048469  \n",
              "4     -0.704387 -0.770272 -1.730537 -1.305973  0.908067 -1.639764  \n",
              "...         ...       ...       ...       ...       ...       ...  \n",
              "10294  0.647372  0.748778  1.730537 -0.127378 -0.477836  0.984414  \n",
              "10295  0.710708  0.714089  1.730874 -0.007074 -0.310328  0.735797  \n",
              "10296  0.531317  0.708206  1.731210  0.080272 -0.344355  0.884723  \n",
              "10297  0.673244  0.681904  1.731546 -0.539951 -0.470604  1.386772  \n",
              "10298  0.829370  0.690000  1.731883 -0.327348 -0.794923  1.137309  \n",
              "\n",
              "[10299 rows x 20 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_final_nparray=np.array(X_final)\n",
        "X_temparr=X_final_nparray[:,selected_features]\n",
        "X_temp=pd.DataFrame(X_final_nparray[:,selected_features])\n",
        "X_temp.head(10299)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCdkgJvhkLOH",
        "outputId": "dd8b4da0-8743-4d6a-c524-4d8a6a43df68"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/200], Train Loss: 0.9357, Train Accuracy: 0.6067, Test Accuracy: 0.7413\n",
            "Epoch [2/200], Train Loss: 0.4693, Train Accuracy: 0.8137, Test Accuracy: 0.8408\n",
            "Epoch [3/200], Train Loss: 0.3474, Train Accuracy: 0.8635, Test Accuracy: 0.8607\n",
            "Epoch [4/200], Train Loss: 0.2846, Train Accuracy: 0.8829, Test Accuracy: 0.8922\n",
            "Epoch [5/200], Train Loss: 0.2510, Train Accuracy: 0.9024, Test Accuracy: 0.8971\n",
            "Epoch [6/200], Train Loss: 0.2340, Train Accuracy: 0.9048, Test Accuracy: 0.9107\n",
            "Epoch [7/200], Train Loss: 0.2263, Train Accuracy: 0.9074, Test Accuracy: 0.9019\n",
            "Epoch [8/200], Train Loss: 0.1970, Train Accuracy: 0.9226, Test Accuracy: 0.9175\n",
            "Epoch [9/200], Train Loss: 0.1887, Train Accuracy: 0.9245, Test Accuracy: 0.9029\n",
            "Epoch [10/200], Train Loss: 0.1910, Train Accuracy: 0.9223, Test Accuracy: 0.9126\n",
            "Epoch [11/200], Train Loss: 0.1781, Train Accuracy: 0.9298, Test Accuracy: 0.9189\n",
            "Epoch [12/200], Train Loss: 0.1540, Train Accuracy: 0.9375, Test Accuracy: 0.9282\n",
            "Epoch [13/200], Train Loss: 0.1505, Train Accuracy: 0.9380, Test Accuracy: 0.9218\n",
            "Epoch [14/200], Train Loss: 0.1456, Train Accuracy: 0.9406, Test Accuracy: 0.9350\n",
            "Epoch [15/200], Train Loss: 0.1327, Train Accuracy: 0.9462, Test Accuracy: 0.9286\n",
            "Epoch [16/200], Train Loss: 0.1282, Train Accuracy: 0.9481, Test Accuracy: 0.9350\n",
            "Epoch [17/200], Train Loss: 0.1277, Train Accuracy: 0.9478, Test Accuracy: 0.9320\n",
            "Epoch [18/200], Train Loss: 0.1187, Train Accuracy: 0.9517, Test Accuracy: 0.9422\n",
            "Epoch [19/200], Train Loss: 0.1052, Train Accuracy: 0.9573, Test Accuracy: 0.9000\n",
            "Epoch [20/200], Train Loss: 0.1032, Train Accuracy: 0.9610, Test Accuracy: 0.9379\n",
            "Epoch [21/200], Train Loss: 0.1004, Train Accuracy: 0.9610, Test Accuracy: 0.9461\n",
            "Epoch [22/200], Train Loss: 0.0917, Train Accuracy: 0.9650, Test Accuracy: 0.9306\n",
            "Epoch [23/200], Train Loss: 0.0834, Train Accuracy: 0.9670, Test Accuracy: 0.9417\n",
            "Epoch [24/200], Train Loss: 0.0877, Train Accuracy: 0.9669, Test Accuracy: 0.9393\n",
            "Epoch [25/200], Train Loss: 0.0788, Train Accuracy: 0.9703, Test Accuracy: 0.9466\n",
            "Epoch [26/200], Train Loss: 0.0784, Train Accuracy: 0.9703, Test Accuracy: 0.9490\n",
            "Epoch [27/200], Train Loss: 0.0629, Train Accuracy: 0.9752, Test Accuracy: 0.9485\n",
            "Epoch [28/200], Train Loss: 0.0640, Train Accuracy: 0.9748, Test Accuracy: 0.9544\n",
            "Epoch [29/200], Train Loss: 0.0538, Train Accuracy: 0.9800, Test Accuracy: 0.9490\n",
            "Epoch [30/200], Train Loss: 0.0526, Train Accuracy: 0.9788, Test Accuracy: 0.9451\n",
            "Epoch [31/200], Train Loss: 0.0694, Train Accuracy: 0.9728, Test Accuracy: 0.9320\n",
            "Epoch [32/200], Train Loss: 0.0559, Train Accuracy: 0.9779, Test Accuracy: 0.9539\n",
            "Epoch [33/200], Train Loss: 0.0431, Train Accuracy: 0.9841, Test Accuracy: 0.9612\n",
            "Epoch [34/200], Train Loss: 0.0375, Train Accuracy: 0.9862, Test Accuracy: 0.9583\n",
            "Epoch [35/200], Train Loss: 0.0410, Train Accuracy: 0.9834, Test Accuracy: 0.9612\n",
            "Epoch [36/200], Train Loss: 0.0336, Train Accuracy: 0.9873, Test Accuracy: 0.9544\n",
            "Epoch [37/200], Train Loss: 0.0399, Train Accuracy: 0.9857, Test Accuracy: 0.9471\n",
            "Epoch [38/200], Train Loss: 0.0394, Train Accuracy: 0.9852, Test Accuracy: 0.9529\n",
            "Epoch [39/200], Train Loss: 0.0467, Train Accuracy: 0.9817, Test Accuracy: 0.9456\n",
            "Epoch [40/200], Train Loss: 0.0623, Train Accuracy: 0.9768, Test Accuracy: 0.9563\n",
            "Epoch [41/200], Train Loss: 0.0345, Train Accuracy: 0.9875, Test Accuracy: 0.9587\n",
            "Epoch [42/200], Train Loss: 0.0257, Train Accuracy: 0.9909, Test Accuracy: 0.9607\n",
            "Epoch [43/200], Train Loss: 0.0223, Train Accuracy: 0.9915, Test Accuracy: 0.9602\n",
            "Epoch [44/200], Train Loss: 0.0186, Train Accuracy: 0.9930, Test Accuracy: 0.9612\n",
            "Epoch [45/200], Train Loss: 0.0361, Train Accuracy: 0.9869, Test Accuracy: 0.9597\n",
            "Epoch [46/200], Train Loss: 0.0286, Train Accuracy: 0.9890, Test Accuracy: 0.9563\n",
            "Epoch [47/200], Train Loss: 0.0192, Train Accuracy: 0.9924, Test Accuracy: 0.9660\n",
            "Epoch [48/200], Train Loss: 0.0201, Train Accuracy: 0.9925, Test Accuracy: 0.9583\n",
            "Epoch [49/200], Train Loss: 0.0227, Train Accuracy: 0.9920, Test Accuracy: 0.9578\n",
            "Epoch [50/200], Train Loss: 0.0192, Train Accuracy: 0.9934, Test Accuracy: 0.9558\n",
            "Epoch [51/200], Train Loss: 0.0248, Train Accuracy: 0.9894, Test Accuracy: 0.9602\n",
            "Epoch [52/200], Train Loss: 0.0273, Train Accuracy: 0.9896, Test Accuracy: 0.9597\n",
            "Epoch [53/200], Train Loss: 0.0290, Train Accuracy: 0.9890, Test Accuracy: 0.9539\n",
            "Epoch [54/200], Train Loss: 0.0257, Train Accuracy: 0.9898, Test Accuracy: 0.9529\n",
            "Epoch [55/200], Train Loss: 0.0242, Train Accuracy: 0.9911, Test Accuracy: 0.9602\n",
            "Epoch [56/200], Train Loss: 0.0185, Train Accuracy: 0.9937, Test Accuracy: 0.9607\n",
            "Epoch [57/200], Train Loss: 0.0081, Train Accuracy: 0.9972, Test Accuracy: 0.9646\n",
            "Epoch [58/200], Train Loss: 0.0068, Train Accuracy: 0.9975, Test Accuracy: 0.9650\n",
            "Epoch [59/200], Train Loss: 0.0098, Train Accuracy: 0.9965, Test Accuracy: 0.9621\n",
            "Epoch [60/200], Train Loss: 0.0145, Train Accuracy: 0.9954, Test Accuracy: 0.9631\n",
            "Epoch [61/200], Train Loss: 0.0110, Train Accuracy: 0.9951, Test Accuracy: 0.9612\n",
            "Epoch [62/200], Train Loss: 0.0209, Train Accuracy: 0.9925, Test Accuracy: 0.9607\n",
            "Epoch [63/200], Train Loss: 0.0075, Train Accuracy: 0.9979, Test Accuracy: 0.9558\n",
            "Epoch [64/200], Train Loss: 0.0146, Train Accuracy: 0.9951, Test Accuracy: 0.9621\n",
            "Epoch [65/200], Train Loss: 0.0095, Train Accuracy: 0.9970, Test Accuracy: 0.9675\n",
            "Epoch [66/200], Train Loss: 0.0086, Train Accuracy: 0.9966, Test Accuracy: 0.9549\n",
            "Epoch [67/200], Train Loss: 0.0126, Train Accuracy: 0.9958, Test Accuracy: 0.9597\n",
            "Epoch [68/200], Train Loss: 0.0472, Train Accuracy: 0.9842, Test Accuracy: 0.9481\n",
            "Epoch [69/200], Train Loss: 0.0255, Train Accuracy: 0.9911, Test Accuracy: 0.9650\n",
            "Epoch [70/200], Train Loss: 0.0088, Train Accuracy: 0.9971, Test Accuracy: 0.9636\n",
            "Epoch [71/200], Train Loss: 0.0061, Train Accuracy: 0.9979, Test Accuracy: 0.9612\n",
            "Epoch [72/200], Train Loss: 0.0055, Train Accuracy: 0.9982, Test Accuracy: 0.9612\n",
            "Epoch [73/200], Train Loss: 0.0216, Train Accuracy: 0.9937, Test Accuracy: 0.9587\n",
            "Epoch [74/200], Train Loss: 0.0121, Train Accuracy: 0.9960, Test Accuracy: 0.9602\n",
            "Epoch [75/200], Train Loss: 0.0093, Train Accuracy: 0.9971, Test Accuracy: 0.9583\n",
            "Epoch [76/200], Train Loss: 0.0149, Train Accuracy: 0.9951, Test Accuracy: 0.9626\n",
            "Epoch [77/200], Train Loss: 0.0245, Train Accuracy: 0.9913, Test Accuracy: 0.9544\n",
            "Epoch [78/200], Train Loss: 0.0198, Train Accuracy: 0.9936, Test Accuracy: 0.9592\n",
            "Epoch [79/200], Train Loss: 0.0058, Train Accuracy: 0.9985, Test Accuracy: 0.9587\n",
            "Epoch [80/200], Train Loss: 0.0037, Train Accuracy: 0.9992, Test Accuracy: 0.9680\n",
            "Epoch [81/200], Train Loss: 0.0023, Train Accuracy: 0.9993, Test Accuracy: 0.9675\n",
            "Epoch [82/200], Train Loss: 0.0026, Train Accuracy: 0.9989, Test Accuracy: 0.9675\n",
            "Epoch [83/200], Train Loss: 0.0068, Train Accuracy: 0.9978, Test Accuracy: 0.9646\n",
            "Epoch [84/200], Train Loss: 0.0055, Train Accuracy: 0.9983, Test Accuracy: 0.9626\n",
            "Epoch [85/200], Train Loss: 0.0238, Train Accuracy: 0.9931, Test Accuracy: 0.9461\n",
            "Epoch [86/200], Train Loss: 0.0388, Train Accuracy: 0.9863, Test Accuracy: 0.9607\n",
            "Epoch [87/200], Train Loss: 0.0157, Train Accuracy: 0.9945, Test Accuracy: 0.9558\n",
            "Epoch [88/200], Train Loss: 0.0108, Train Accuracy: 0.9962, Test Accuracy: 0.9617\n",
            "Epoch [89/200], Train Loss: 0.0045, Train Accuracy: 0.9990, Test Accuracy: 0.9636\n",
            "Epoch [90/200], Train Loss: 0.0187, Train Accuracy: 0.9934, Test Accuracy: 0.9612\n",
            "Epoch [91/200], Train Loss: 0.0096, Train Accuracy: 0.9973, Test Accuracy: 0.9650\n",
            "Epoch [92/200], Train Loss: 0.0031, Train Accuracy: 0.9993, Test Accuracy: 0.9655\n",
            "Epoch [93/200], Train Loss: 0.0022, Train Accuracy: 0.9995, Test Accuracy: 0.9684\n",
            "Epoch [94/200], Train Loss: 0.0013, Train Accuracy: 0.9998, Test Accuracy: 0.9689\n",
            "Epoch [95/200], Train Loss: 0.0011, Train Accuracy: 0.9998, Test Accuracy: 0.9704\n",
            "Epoch [96/200], Train Loss: 0.0008, Train Accuracy: 0.9998, Test Accuracy: 0.9675\n",
            "Epoch [97/200], Train Loss: 0.0013, Train Accuracy: 0.9996, Test Accuracy: 0.9694\n",
            "Epoch [98/200], Train Loss: 0.0023, Train Accuracy: 0.9993, Test Accuracy: 0.9646\n",
            "Epoch [99/200], Train Loss: 0.0428, Train Accuracy: 0.9846, Test Accuracy: 0.9524\n",
            "Epoch [100/200], Train Loss: 0.0240, Train Accuracy: 0.9916, Test Accuracy: 0.9592\n",
            "Epoch [101/200], Train Loss: 0.0110, Train Accuracy: 0.9967, Test Accuracy: 0.9592\n",
            "Epoch [102/200], Train Loss: 0.0128, Train Accuracy: 0.9966, Test Accuracy: 0.9675\n",
            "Epoch [103/200], Train Loss: 0.0103, Train Accuracy: 0.9967, Test Accuracy: 0.9539\n",
            "Epoch [104/200], Train Loss: 0.0089, Train Accuracy: 0.9977, Test Accuracy: 0.9650\n",
            "Epoch [105/200], Train Loss: 0.0024, Train Accuracy: 0.9994, Test Accuracy: 0.9670\n",
            "Epoch [106/200], Train Loss: 0.0027, Train Accuracy: 0.9994, Test Accuracy: 0.9655\n",
            "Epoch [107/200], Train Loss: 0.0018, Train Accuracy: 0.9994, Test Accuracy: 0.9684\n",
            "Epoch [108/200], Train Loss: 0.0007, Train Accuracy: 0.9999, Test Accuracy: 0.9675\n",
            "Epoch [109/200], Train Loss: 0.0005, Train Accuracy: 0.9999, Test Accuracy: 0.9680\n",
            "Epoch [110/200], Train Loss: 0.0005, Train Accuracy: 0.9999, Test Accuracy: 0.9680\n",
            "Epoch [111/200], Train Loss: 0.0004, Train Accuracy: 0.9999, Test Accuracy: 0.9684\n",
            "Epoch [112/200], Train Loss: 0.0004, Train Accuracy: 0.9999, Test Accuracy: 0.9684\n",
            "Epoch [113/200], Train Loss: 0.0004, Train Accuracy: 0.9999, Test Accuracy: 0.9694\n",
            "Epoch [114/200], Train Loss: 0.0002, Train Accuracy: 1.0000, Test Accuracy: 0.9699\n",
            "Epoch [115/200], Train Loss: 0.0002, Train Accuracy: 1.0000, Test Accuracy: 0.9689\n",
            "Epoch [116/200], Train Loss: 0.0002, Train Accuracy: 1.0000, Test Accuracy: 0.9714\n",
            "Epoch [117/200], Train Loss: 0.0002, Train Accuracy: 1.0000, Test Accuracy: 0.9699\n",
            "Epoch [118/200], Train Loss: 0.0001, Train Accuracy: 1.0000, Test Accuracy: 0.9684\n",
            "Epoch [119/200], Train Loss: 0.0001, Train Accuracy: 1.0000, Test Accuracy: 0.9684\n",
            "Epoch [120/200], Train Loss: 0.0002, Train Accuracy: 1.0000, Test Accuracy: 0.9694\n",
            "Epoch [121/200], Train Loss: 0.0001, Train Accuracy: 1.0000, Test Accuracy: 0.9699\n",
            "Epoch [122/200], Train Loss: 0.0001, Train Accuracy: 1.0000, Test Accuracy: 0.9699\n",
            "Epoch [123/200], Train Loss: 0.0001, Train Accuracy: 1.0000, Test Accuracy: 0.9694\n",
            "Epoch [124/200], Train Loss: 0.0001, Train Accuracy: 1.0000, Test Accuracy: 0.9689\n",
            "Epoch [125/200], Train Loss: 0.0001, Train Accuracy: 1.0000, Test Accuracy: 0.9689\n",
            "Epoch [126/200], Train Loss: 0.0001, Train Accuracy: 1.0000, Test Accuracy: 0.9694\n",
            "Epoch [127/200], Train Loss: 0.0001, Train Accuracy: 1.0000, Test Accuracy: 0.9689\n",
            "Epoch [128/200], Train Loss: 0.0001, Train Accuracy: 1.0000, Test Accuracy: 0.9680\n",
            "Epoch [129/200], Train Loss: 0.0001, Train Accuracy: 1.0000, Test Accuracy: 0.9684\n",
            "Epoch [130/200], Train Loss: 0.0001, Train Accuracy: 1.0000, Test Accuracy: 0.9680\n",
            "Epoch [131/200], Train Loss: 0.0001, Train Accuracy: 1.0000, Test Accuracy: 0.9680\n",
            "Epoch [132/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9675\n",
            "Epoch [133/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9684\n",
            "Epoch [134/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9684\n",
            "Epoch [135/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9689\n",
            "Epoch [136/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9684\n",
            "Epoch [137/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9684\n",
            "Epoch [138/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9680\n",
            "Epoch [139/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9684\n",
            "Epoch [140/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9684\n",
            "Epoch [141/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9689\n",
            "Epoch [142/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9689\n",
            "Epoch [143/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9680\n",
            "Epoch [144/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9680\n",
            "Epoch [145/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9694\n",
            "Epoch [146/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9689\n",
            "Epoch [147/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9684\n",
            "Epoch [148/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9675\n",
            "Epoch [149/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9689\n",
            "Epoch [150/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9694\n",
            "Epoch [151/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9675\n",
            "Epoch [152/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9689\n",
            "Epoch [153/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9684\n",
            "Epoch [154/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9684\n",
            "Epoch [155/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9680\n",
            "Epoch [156/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9694\n",
            "Epoch [157/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9680\n",
            "Epoch [158/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9689\n",
            "Epoch [159/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9680\n",
            "Epoch [160/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9680\n",
            "Epoch [161/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9675\n",
            "Epoch [162/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9684\n",
            "Epoch [163/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9689\n",
            "Epoch [164/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9680\n",
            "Epoch [165/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9689\n",
            "Epoch [166/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9684\n",
            "Epoch [167/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9684\n",
            "Epoch [168/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9680\n",
            "Epoch [169/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9694\n",
            "Epoch [170/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9689\n",
            "Epoch [171/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9680\n",
            "Epoch [172/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9694\n",
            "Epoch [173/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9684\n",
            "Epoch [174/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9680\n",
            "Epoch [175/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9689\n",
            "Epoch [176/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9694\n",
            "Epoch [177/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9684\n",
            "Epoch [178/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9694\n",
            "Epoch [179/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9680\n",
            "Epoch [180/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9689\n",
            "Epoch [181/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9689\n",
            "Epoch [182/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9689\n",
            "Epoch [183/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9694\n",
            "Epoch [184/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9694\n",
            "Epoch [185/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9689\n",
            "Epoch [186/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9704\n",
            "Epoch [187/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9694\n",
            "Epoch [188/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9704\n",
            "Epoch [189/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9694\n",
            "Epoch [190/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9704\n",
            "Epoch [191/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9699\n",
            "Epoch [192/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9704\n",
            "Epoch [193/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9694\n",
            "Epoch [194/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9689\n",
            "Epoch [195/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9704\n",
            "Epoch [196/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9699\n",
            "Epoch [197/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9704\n",
            "Epoch [198/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9704\n",
            "Epoch [199/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9704\n",
            "Epoch [200/200], Train Loss: 0.0000, Train Accuracy: 1.0000, Test Accuracy: 0.9699\n",
            "Final Testing Accuracy: 0.9699029126213592\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_combined_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "X_tensor = torch.FloatTensor(X_temparr)\n",
        "y_tensor = torch.LongTensor(y_combined_encoded)\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "X_tensor, y_tensor = X_tensor.to(device), y_tensor.to(device)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "class HybridModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(HybridModel, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3)\n",
        "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.lstm1 = nn.LSTM(input_size=128, hidden_size=hidden_size, batch_first=True, dropout=0.5)\n",
        "        self.lstm2 = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, batch_first=True, dropout=0.5)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x.unsqueeze(1)))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x, _ = self.lstm1(x)\n",
        "        x, _ = self.lstm2(x)\n",
        "        x = x[:, -1, :]\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "model = HybridModel(input_size=20, hidden_size=64, num_classes=len(label_encoder.classes_)).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "num_epochs = 200\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct_train_predictions = 0\n",
        "    total_train_samples = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train_samples += labels.size(0)\n",
        "        correct_train_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "    train_epoch_loss = total_loss / len(train_loader)\n",
        "    train_epoch_accuracy = correct_train_predictions / total_train_samples\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    correct_test_predictions = 0\n",
        "    total_test_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            total_test_samples += labels.size(0)\n",
        "            correct_test_predictions += (preds == labels).sum().item()\n",
        "\n",
        "    test_epoch_accuracy = correct_test_predictions / total_test_samples\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
        "          f\"Train Loss: {train_epoch_loss:.4f}, Train Accuracy: {train_epoch_accuracy:.4f}, \"\n",
        "          f\"Test Accuracy: {test_epoch_accuracy:.4f}\")\n",
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "final_accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f\"Final Testing Accuracy: {final_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oULCR8gFkRWO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
